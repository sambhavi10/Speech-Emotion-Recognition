# -*- coding: utf-8 -*-
"""SpeechEmotionModel(CNN MEL).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xZUzwSBSlCP1NQaEb9AJeWJkMaFus3wk
"""

import librosa
import librosa.display
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from matplotlib.pyplot import specgram
import keras
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Input, Flatten, Dropout, Activation
from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D
from keras.models import Model
from keras.callbacks import ModelCheckpoint
from sklearn.metrics import confusion_matrix

# from google.colab import drive
# drive.mount('/content/drive')

pip install python_speech_features

from tensorflow.keras.utils import to_categorical

from keras import regularizers
import os
from tqdm import tqdm 
from scipy.io import wavfile
from python_speech_features import mfcc, logfbank 
import pandas as pd

import glob

"""### loading csv file"""

df = pd.read_csv('/content/drive/MyDrive/Minor Project files/TrainingDataset.csv')

# from google.colab import drive
# drive.mount('/content/drive')

df.head()

"""### Converting Spectrogram into image files"""

# from google.colab import drive
# drive.mount('/content/drive')

data_dir = 'drive/MyDrive/MEL/Train'
batch_size = 32
img_height = 180
img_width = 180
train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  color_mode="rgb",
  subset="training",
  seed = 123,
  image_size=(img_height, img_width),
  batch_size=batch_size
)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  color_mode="rgb",
  subset="validation",
  seed=123,
  image_size = (img_height, img_width),
  batch_size = batch_size)

class_names = train_ds.class_names
print(class_names)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

normalization_layer = tf.keras.layers.Rescaling(1./255)

normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))

num_classes = 4

model = tf.keras.Sequential([
  tf.keras.layers.Rescaling(1./255),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(num_classes, activation = 'softmax')
])

model.compile(
  optimizer='adam',
  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

# model.summary()S

history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs = 50
)

# score = model.evaluate(x_test, y_test, verbose=0)
# print(model.metrics_names[1], score[1])

# trainscore = model.evaluate(x_train, y_train, verbose=0)
# print(model.metrics_names[1], trainscore[1])

model.summary()

import pickle

from keras.preprocessing.image import ImageDataGenerator

test_generator = ImageDataGenerator()
test_data_generator = test_generator.flow_from_directory(
    'drive/MyDrive/MEL/Test', # Put your path here
     target_size=(img_width, img_height),
    batch_size=32,
    shuffle=False)
test_steps_per_epoch = np.math.ceil(test_data_generator.samples / test_data_generator.batch_size)

predictions = model.predict_generator(test_data_generator, steps=test_steps_per_epoch)
# Get most likely class
predicted_classes = np.argmax(predictions, axis=1)

print(predictions)

print(predicted_classes)

CNN_MEL_df = pd.DataFrame(predicted_classes)

CNN_MEL_df

CNN_MEL_df.to_csv("CNN_MEL_df.csv")

# from google.colab import drive
# drive.mount('/content/drive')

true_classes = test_data_generator.classes
class_labels = list(test_data_generator.class_indices.keys())

true_classes

class_labels

model.save("/content/drive/MyDrive/Minor Project files/", save_format = '.h5')
f = open("/content/drive/MyDrive/Minor Project files/model.pkl","wb")
f.write(pickle.dumps(class_labels))
f.close()

from sklearn import metrics
report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)
print(report)

from sklearn.metrics import accuracy_score
acc = accuracy_score(true_classes, predicted_classes)
acc

res = tf.math.confusion_matrix(true_classes,predicted_classes)
res

import seaborn as sns
ax = sns.heatmap(res, annot=True, cmap='Blues')
ax.set_title('CNN Using MFCC features\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
plt.show()

import matplotlib.pyplot as plt

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()