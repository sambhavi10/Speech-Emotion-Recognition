# -*- coding: utf-8 -*-
"""URDU ML Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12S83hJ1NmoKr4S2al7yAYMYV6DtuHZNh
"""

import librosa
import librosa.display
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf4
from matplotlib.pyplot import specgram
from sklearn.metrics import confusion_matrix

pip install python_speech_features

from tensorflow.keras.utils import to_categorical

from keras import regularizers
import os
from tqdm import tqdm 
from scipy.io import wavfile
from python_speech_features import mfcc, logfbank 
import pandas as pd

import glob

"""### loading csv file"""

df = pd.read_csv("/content/drive/MyDrive/Minor Project files/csv files/URDU csv file.csv")

df.head()

import seaborn as sns
result = df.groupby(['Emotion Class']).size()
 
# plot the result
sns.barplot(x = result.index, y = result.values)

df.shape

data, sampling_rate = librosa.load('drive/MyDrive/URDU audio files/S01.wav')

plt.figure(figsize=(10, 5))
librosa.display.waveplot(data, sr = 44100)

audioFileList = df['File Name'].tolist()

len(audioFileList)

len(audioFileList)

print(audioFileList[300])

"""## creating emotion list based on pattern in audio file names"""

emotion_list = []
i = 0
for item in audioFileList:

  if audioFileList[i][0:1] == 'S':
    emotion_list.append('Sad')
  elif audioFileList[i][0:1] == 'A':
    emotion_list.append('Angry')
  elif audioFileList[i][0:1] == 'H':
    emotion_list.append('Happy')
  elif audioFileList[i][0:1] == 'N':
    emotion_list.append('Neutral')
  i+=1

print(emotion_list)

print(len(emotion_list))

emotion_classes_df = pd.DataFrame(emotion_list,columns=['emotion_class'])
emotion_classes_df.head()

emotion_classes_df.shape



"""### Converting the MFCCs into features"""

df1 = pd.DataFrame(columns=['features'])
bookmark1 = 0
for index,y in enumerate(audioFileList):
  X, sampling_rate = librosa.load('drive/My Drive/URDU audio files/'+y, duration = 2.5, res_type='kaiser_fast',sr=22050*2,offset=0.5)
  sampling_rate = np.array(sampling_rate)
  mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sampling_rate, n_mfcc=128), axis=0)
  features = mfccs
  df1.loc[bookmark1] = [features]
  bookmark1=bookmark1+1

df1.head()

df1.shape

len(df1['features'][1]) #these many features in a feature vector

df2 = pd.DataFrame(df1['features'].values.tolist())

df2.head()

newdf2 = pd.concat([df2,emotion_classes_df], axis = 1)

newdf2.head()

"""### Label Encoding"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
newdf_copy = newdf2 #making a copy

#converted into 3 unique values
Emotion_train = le.fit_transform(newdf2.emotion_class)
Emotion_train

le.classes_

newdf2.emotion_class= le.fit_transform(newdf2.emotion_class)
newdf2

"""handling NaN valiue"""

newdf2 = newdf2.fillna(0)

newdf2.head()

X = newdf2.iloc[:,:-1]

X.head()

Y = newdf2.iloc[:,-1]

"""### Train Test Split"""

import sklearn
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

#Just For ANN Model, using 7 different classes shaped (7)
from keras.utils import np_utils
from sklearn.preprocessing import LabelEncoder

x_train_ANN = np.array(x_train)
y_train_ANN = np.array(y_train)
x_test_ANN = np.array(x_test)
y_test_ANN = np.array(y_test)

le = LabelEncoder()

y_train_ANN = np_utils.to_categorical(le.fit_transform(y_train_ANN))
y_test_ANN = np_utils.to_categorical(le.fit_transform(y_test_ANN))

y_train_ANN.shape



"""## Random Forest Model"""

#feature Scaling  
from sklearn.preprocessing import StandardScaler    
st_x = StandardScaler()    
x_train = st_x.fit_transform(x_train)    
x_test = st_x.transform(x_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB

rf = RandomForestClassifier(n_estimators = 120, criterion = 'entropy', random_state =0)
rf.fit(x_train, y_train)

#Predicting the test set result  
y_pred_rf = rf.predict(x_test)

y_pred_rf

#Creating the Confusion matrix  
from sklearn.metrics import confusion_matrix  
cm = confusion_matrix(y_test, y_pred_rf)

cm

import seaborn as sns
ax = sns.heatmap(cm, annot=True, cmap='Blues')
ax.set_title('Random Forest\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
plt.show()

from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_pred_rf)
acc

import sklearn.metrics as metrics
print(metrics.classification_report(y_test, y_pred_rf))

rf_df = pd.DataFrame(y_pred_rf)
rf_df.to_csv("rf_df.csv")

"""### Naive Bayes"""

nb = GaussianNB()
nb.fit(x_train, y_train) 
y_pred_nb = nb.predict(x_test)
from sklearn.metrics import confusion_matrix, balanced_accuracy_score 
cm = confusion_matrix(y_test, y_pred_nb)
cm

import seaborn as sns
ax = sns.heatmap(cm, annot=True, cmap='Blues')
ax.set_title('Naive Bayes\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
plt.show()

acc = accuracy_score(y_test, y_pred_nb)
acc

import sklearn.metrics as metrics
print(metrics.classification_report(y_test, y_pred_nb))

"""##Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(solver = 'saga',random_state = 0, penalty = 'l2', max_iter=80)
classifier.fit(x_train, y_train)

#Predicting the test set result  
y_predLR = classifier.predict(x_test)

#Creating the Confusion matrix  
from sklearn.metrics import confusion_matrix  
cm = confusion_matrix(y_test, y_predLR)

cm

import seaborn as sns
ax = sns.heatmap(cm, annot=True, cmap='Blues')
ax.set_title('Logistic Regression\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
plt.show()

from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_predLR)
acc

import sklearn.metrics as metrics
print(metrics.classification_report(y_test, y_predLR))

"""### SVM Model"""

from sklearn.svm import SVC # "Support vector classifier"  
classifier = SVC(C = 1, random_state=24, probability = True)  
classifier.fit(x_train, y_train)

#Predicting the test set result  
y_predSVM = classifier.predict(x_test)

#Creating the Confusion matrix  
from sklearn.metrics import confusion_matrix  
cm = confusion_matrix(y_test, y_predSVM)

cm

y_predSVM

import seaborn as sns
ax = sns.heatmap(cm, annot=True, cmap='Blues')
ax.set_title('SVM\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
plt.show()

from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_predSVM)
acc

import sklearn.metrics as metrics
print(metrics.classification_report(y_test, y_predSVM))



"""### XGBoost"""

# from xgboost import XGBClassifier
# # fit model no training data
# model4 = XGBClassifier(n_estimators=20, learning_rate=0.1, max_depth=3, random_state=0)
# model4.fit(x_train, y_train)

# predXG = model4.predict(x_test)

# acc = accuracy_score(y_test, predXG)
# acc

# print(metrics.classification_report(y_test, predXG))

# #Creating the Confusion matrix  
# from sklearn.metrics import confusion_matrix  
# cm = confusion_matrix(y_test, predXG)

# cm



"""### Ensemble"""

model1 = RandomForestClassifier(n_estimators = 120, criterion = 'entropy', random_state =0)
model2 = GaussianNB()
model3 = SVC(C = 1, random_state=24, probability = True)    

model1.fit(x_train,y_train)
model2.fit(x_train,y_train)
model3.fit(x_train,y_train)

pred1=model1.predict_proba(x_test)
pred2=model2.predict_proba(x_test)
pred3=model3.predict_proba(x_test)
finalpred = (pred1 * 0.55 + pred2 * 0.05 + pred3 * 0.45)

# finalpred

finalpreds = finalpred.argmax(axis=1)
finalpreds

import sklearn.metrics as metrics
print(metrics.classification_report(y_test, finalpreds))

acc = accuracy_score(y_test, finalpreds)
acc

from sklearn.metrics import confusion_matrix  
cm = confusion_matrix(y_test, finalpreds)

cm

import seaborn as sns
ax = sns.heatmap(cm, annot=True, cmap='Blues')
ax.set_title('Ensemble Model\n\n');
ax.set_xlabel('\nPredicted Values')
ax.set_ylabel('Actual Values ');
plt.show()

ensemble_df = pd.DataFrame(finalpreds)

ensemble_df.to_csv('ensemble_df.csv')

y_test_df = pd.DataFrame(y_test)
y_test_df.to_csv("y_test_df.csv")

from sklearn.metrics import roc_curve, roc_auc_score

def ROC_curve(model, X_test, y_test, save_name, save, dpi_ = 300):
  y_pred_proba = model.predict_proba(x_test)
  AUC_score_model = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')
  
  fpr = {}
  tpr = {}
  thresh ={}
  n_classes = 4

  for i in range(n_classes):    
      fpr[i], tpr[i], thresh[i] = roc_curve(y_test, y_pred_proba[:,i], pos_label=i)

    
# plotting    
  plt.plot(fpr[0], tpr[0], linestyle='solid',color='orange', label='ROC Angry')
  plt.plot(fpr[1], tpr[1], linestyle='solid',color='green', label='ROC Happy')
  plt.plot(fpr[2], tpr[2], linestyle='solid',color='blue', label='ROC Neutral')
  plt.plot(fpr[3], tpr[3], linestyle='solid',color='yellow', label='ROC Sad')
  plt.plot([0, 1], [0, 1], color="black", lw=2, linestyle="--")


  plt.title(save_name)
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive rate')
  plt.legend(loc='best')
  if save == True:
    plt.savefig(save_name,dpi=dpi_);    

  return AUC_score_model,   [fpr, tpr, thresh]

roc_rf = ROC_curve(rf, x_test, y_test, "rf.jpeg", save=True)

roc_nb = ROC_curve(nb, x_test, y_test, "nb.jpeg", save=True)

roc_lr = ROC_curve(classifier, x_test, y_test,"lr.jpeg",  save=True)

roc_svm = ROC_curve(classifier, x_test, y_test, "svm.jpeg", save=True)

